---
title: "Introduction to `simulatr`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to `simulatr`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE,
  comment = "#>"
)
```

Here we introduce the data structures on which `simulatr` is based and walk through a simple example of using these to set up a numerical simulation.

```{r setup, message = FALSE}
library(simulatr)
library(tidyverse)
```

# Simulation structure and workflow

## The conceptual structure of a numerical simulation

The goal of a numerical simulation is to evaluate the performance of one or more statistical methods on data generated from one or more data-generating distributions. 

- **Problem parameters and inferential targets.** Each data-generating distribution is indexed by a number of *problem parameters*, e.g. the sample size or dimension of the problem, collectively called a *problem setting*. The problem parameters give rise to ground truth values of *inferential targets*, e.g. coefficients in a regression. The mapping between problem parameters and ground truth inferential target values, *inferential target generation*, may be either deterministic or randomized. 

- **Data generation.** The problem parameters and inferential target values give rise to a data-generating distribution. These, together with *simulation parameters* (e.g. the number of Monte Carlo realizations `B` to generate) are plugged into the *data generation* function to get data realizations for each data-generating distribution. 

- **Method application.** Each of the *methods* functions is applied to each realization of each data-generating distribution to give a number of *method outputs*. These outputs should include values for each inferential target (e.g. coefficient estimates). 

- **Method evaluation.** Then, one or more *evaluation functions* take as input the ground truth inferential target values and method outputs and compute some metric of inferential accuracy (e.g. root-mean-squared error). These *evaluation results* for each method and each problem setting, averaged over `B` Monte Carlo realizations, are the final output of the simulation study.

```{r, echo = FALSE, out.width = '100%'}
knitr::include_graphics("../man/figures/simulatr-schematic.png")
```

## `simulatr` specifier objects

In `simulatr`, the aforementioned components are all captured by a *`simulatr` specifier object*. This object has five fields, which are described below:

- `parameter_grid`: A data frame whose rows correspond to problem settings and whose columns are problem parameters. Additionally, the `parameter_grid` contains a special column called `ground_truth`, which contains an object of ground truth inferential target values for each problem setting (implemented as a [list-column](https://r4ds.hadley.nz/rectangling.html#list-columns)).
- `fixed_parameters`: An object whose fields are either simulation parameters or problem parameters common to all problem setting. This object must contain at least the fields `B`, the number of data realizations to generate per problem setting, and `seed`, the seed to set prior to the generation of data for each problem setting and prior to the application of each method.
- `generate_data_function`: A function that takes as input the problem parameters and ground truth inferential target values and outputs either (1) one data set or (2) `B` data sets. The latter option is useful in cases where all data sets are faster to generate together rather than one at a time.
- `run_method_functions`: A named list of *method functions*. A method function takes as input either (1) one data set or (2) `B` data sets. In case (1), the function outputs an object that contains fields with names corresponding to the inferential targets. In case (2), the function outputs a data frame with two columns: one column named `run_id` corresponding to the Monte Carlo replicate and one list-column named `output` containing the objects outputted for each replicate.
- `evaluation_functions`: A named list of *evaluation functions*. An evaluation function takes as input two objects: the ground truth inferential targets and the outputs from a method function. It outputs the value of an evaluation metric.

## `simulatr` workflow

1. **Assemble simulation components.** First, assemble the five objects described in the previous section.

2. **Create a `simulatr` specifier object.** Given the four simulation components, create a `simulatr` specifier object using the `simulatr_specifier()` function:
    ```{r, eval = FALSE}
    simulatr_spec <- simulatr_specifier(
      parameter_grid,
      fixed_parameters,
      generate_data_function, 
      run_method_functions,
      evaluation_functions
    )
    ```

3. **Check and, if necessary, update the `simulatr` specifier object.** Make sure that all parts of your `simulatr` specifier object are working by running a small portion of your simulation via the `check_simulatr_specifier_object()` function. This function takes as arguments the `simulatr` specifier object (`simulatr_spec`) as well as the number of data realizations to try (`B_in`). For checking purposes, you can use a small number for `B_in` like 2 or 3.
    ```{r, eval = FALSE}
    check_results <- check_simulatr_specifier_object(simulatr_spec, B_in = 3)
    ```
If there are any errors, `simulatr` will give informative error messages that will let you know which method and which parameter setting caused the problem, along with the corresponding data realization. Update the `simulatr` specifier object to fix any issues.

4. **Run the simulation.** There are two options for running a numerical simulation with `simulatr`. You can run `simulatr` either (1) using `RStudio` or (2) using the `simulatr` *pipeline*, a Nextflow pipeline intended for distributed computing platforms. Option 1, good for small-scale simulations, can be done using the `check_simulatr_specifier_object()` function from step 3, omitting the `B_in` argument:
    ```{r, eval = FALSE}
    simulation_results <- check_simulatr_specifier_object(simulatr_spec)
    ```
Option 2, good for large-scale simulations, is currently available but not yet documented.

5. **Summarize and / or visualize the results.** The `simulatr` output will give you the value of each evaluation metric on each method on each problem setting. You can then create tables or graphs of these results.

# Example simulation with `simulatr`

Let's walk through the above workflow on a simple example. We consider estimating the coefficients in a linear regression model via ordinary least squares and lasso, varying the number of samples. 

## 1. Assemble simulation components

- `parameter_grid`. The problem parameters will be the sample size `n`, the dimension `p`, the number of nonzero coefficients `s`, and the value of each nonzero coefficient `beta_val`. In this simulation, we will only vary `n`. We could have therefore put the remainder of the parameters in `fixed_parameters`, but we avoid this to have all problem parameters in `parameter_grid`.
    ```{r}
    parameter_grid <- data.frame(
      n = c(25, 50, 75, 100),      # sample size
      p = 15,                      # dimension
      s = 5,                       # number of nonzero coefficients
      beta_val = 3                 # value of nonzero coefficients
    )
    ```
Next, we want to create the ground truth inferential targets based on the problem parameters. In this case, the inferential target is the coefficient vector. We define a function `get_ground_truth()` that takes as input three of the problem parameters and outputs the coefficient vector:
    ```{r}
    get_ground_truth <- function(p, s, beta_val){
      beta <- numeric(p)
      beta[1:s] <- beta_val
      list(beta = beta)
    }
    ```
We can append these ground truth inferential targets to `parameter_grid` using the helper function `add_ground_truth()`:
    ```{r}
    parameter_grid <- parameter_grid |> add_ground_truth(get_ground_truth)
    ```
Let's take a look at the resulting parameter grid:
    ```{r}
    parameter_grid
    ```
We see the four columns with problem parameters and the fifth with the ground truth coefficient vectors.

- `fixed_parameters`. Here we put only the number of data realizations `B` and the seed to set for the simulation. 
    ```{r}
    fixed_parameters <- list(
      B = 20,                      # number of data realizations
      seed = 4                    # seed to set prior to generating data and running methods
    )
    ```
    
- `generate_data_function`. We define the data-generation function as follows:
    ```{r}
    # define data-generating model based on the Gaussian linear model
    generate_data_f <- function(n, p, ground_truth){
      X <- matrix(rnorm(n*p), n, p, dimnames = list(NULL, paste0("X", 1:p)))
      y <- X %*% ground_truth$beta + rnorm(n)
      data <- list(X = X, y = y)
      data
    }
    ```
Note that we extract the coefficient vector `beta` from the `ground_truth` object. Additionally, we need to call `simulatr_function()` to add a few pieces of information that `simulatr` needs. In particular, we have to give it the argument names of the data-generating function (typically obtained using `formalArgs(generate_data_f)`, as below). We also have to let `simulatr` know whether the data-generating function creates just one realization of the data (`loop = TRUE`) or all `B` at the same time (`loop = FALSE`).
    ```{r}
    # need to call simulatr_function() to give simulatr a few more pieces of info
    generate_data_function <- simulatr_function(
      f = generate_data_f,                        
      arg_names = formalArgs(generate_data_f),    
      loop = TRUE
    )
    ```

- `run_method_functions`. Let's define functions for OLS and lasso:
    ```{r}
    # ordinary least squares
    ols_f <- function(data){
      X <- data$X
      y <- data$y
      lm_fit <- lm(y ~ X - 1)
      beta_hat <- coef(lm_fit)
      results <- list(beta = unname(beta_hat))
      results
    }
    
    # lasso
    lasso_f <- function(data){
      X <- data$X
      y <- data$y
      glmnet_fit <- glmnet::cv.glmnet(x = X, y = y, nfolds = 5, intercept = FALSE)
      beta_hat <- glmnet::coef.glmnet(glmnet_fit, s = "lambda.1se")
      results <- list(beta = beta_hat[-1])
      results
    }
    ```
    Again, we need to call `simulatr_function()` to add a few pieces of information that `simulatr` needs. This time, we need to give it all arguments to the method functions *except the first* (the data itself), which usually will be empty. We also have to let `simulatr` know whether the method functions input just one realization of the data (`loop = TRUE`) or all `B` at the same time (`loop = FALSE`).
    ```{r}
    # create simulatr functions
    ols_spec_f <- simulatr_function(f = ols_f, arg_names = character(0), loop = TRUE)
    lasso_spec_f <- simulatr_function(f = lasso_f, arg_names = character(0), loop = TRUE)
    ```
    Finally, we collate the above method functions into a named list. It is crucial that the list be named. 
    ```{r}
    run_method_functions <- list(ols = ols_spec_f, lasso = lasso_spec_f)
    ```
  
- `evaluation_functions`. Let's evaluate the estimate of `beta` using the RMSE. To accomplish this, we define the following evaluation function:
    ```{r}
    rmse <- function(output, ground_truth) {
      sqrt(sum((output$beta - ground_truth$beta)^2))
    }
    evaluation_functions <- list(rmse = rmse)
    ```
    
## 2. Create a `simulatr` specifier object

This is the easiest step; just pass all four of the above components to the function `simulatr_specifier()`:
```{r}
simulatr_spec <- simulatr_specifier(
  parameter_grid,
  fixed_parameters,
  generate_data_function, 
  run_method_functions,
  evaluation_functions
)
```

## 3. Check and, if necessary, update the `simulatr` specifier object
```{r}
check_results <- check_simulatr_specifier_object(simulatr_spec, B_in = 2)
```
This message tells us that the simulation did not encounter any errors for the first two data realizations. We are free to move on to running the full simulation.
   
## 4. Run the simulation

Since this example simulation is small, we can run it in RStudio:
```{r}
sim_results <- check_simulatr_specifier_object(simulatr_spec)
```

## 5. Summarize and/or visualize the results

Let's take a look at the results:
```{r}
sim_results$metrics
```
We have both the mean and Monte Carlo standard error for the metric (RMSE) for each method in each problem setting. We can plot these as follows:

```{r, fig.align='center', fig.width=6, fig.height = 4}
sim_results$metrics |>
  ggplot(aes(x = n, 
             y = mean, 
             ymin = mean - 2*se, 
             ymax = mean + 2*se, 
             color = method)) +
  geom_point() + 
  geom_line() + 
  geom_errorbar(width = 1) +
  labs(x = "Sample size",
       y = "RMSE") + 
  theme(legend.position = "bottom")
```

It looks like lasso performs better for small sample sizes but OLS performs better for large sample sizes.